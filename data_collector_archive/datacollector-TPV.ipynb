{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet\n",
    "import time\n",
    "import pybullet_data\n",
    "import pybullet_robots\n",
    "import math\n",
    "import numpy as np\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup constants\n",
    "\n",
    "#width and height for rgb and depth image\n",
    "IMG_WIDTH = 160\n",
    "IMG_HEIGHT = 120\n",
    "\n",
    "#robot constants\n",
    "NUM_JOINTS = 7\n",
    "FINGER_AXES = [9, 10]\n",
    "\n",
    "#data record path\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup global variables\n",
    "runNumber = 0 #folder number to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vr\n",
      "c:\\Users\\RUSHI\\anaconda3\\envs\\tf\\lib\\site-packages\\pybullet_data\n"
     ]
    }
   ],
   "source": [
    "usingVR = True\n",
    "\n",
    "#setup the simulation\n",
    "if usingVR:\n",
    "    physics = pybullet.connect(pybullet.SHARED_MEMORY)\n",
    "    if (physics<0):\n",
    "        print('could not use vr!')\n",
    "    else:\n",
    "        print('using vr')\n",
    "else:\n",
    "    physics = pybullet.connect(pybullet.GUI)\n",
    "\n",
    "#add a plane to the simulation\n",
    "pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "print(pybullet_data.getDataPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomPos(x_range = [-0.6, -0.2], y_range = [-0.2, 0]):\n",
    "        x = np.random.uniform(x_range[0], x_range[1])\n",
    "        y = np.random.uniform(y_range[0], y_range[1])\n",
    "        return [x, y, 0.7]\n",
    "\n",
    "def reset():\n",
    "    # clear all the objects in the simulation\n",
    "    pybullet.resetSimulation()\n",
    "    plainId = pybullet.loadURDF(\"plane.urdf\", [0, 0, 0], useFixedBase=True)\n",
    "\n",
    "    # #add a table in front of the robot arm\n",
    "    tableId = pybullet.loadURDF(\"table/table.urdf\", basePosition=[0,0,0])\n",
    "\n",
    "    #add misc objects to the simulation on the table\n",
    "    #pybullet.loadURDF(\"lego/lego.urdf\", basePosition=[-0.6,-0.2,0.7], globalScaling=2)\n",
    "\n",
    "    #pybullet.loadURDF(\"tray/traybox.urdf\", basePosition=[0.1, -0.3, 0.65], globalScaling=0.5)\n",
    "\n",
    "    #add a robot arm to the simulation\n",
    "    robotId = pybullet.loadURDF(\"franka_panda/panda.urdf\", [-0,0.3,0.6], useFixedBase=True)\n",
    "    # robotId = pybullet.loadURDF(\"kuka_iiwa/model.urdf\", [-0,0.3,0.6], useFixedBase=True)\n",
    "    #set the arm to an experimentally good starting position\n",
    "    initState = [1.976510182790837, -1.4559139655311553, 1.2882844021675286, -1.572705259702399, 1.3237965931867992, 1.110313401859266, 2.873038394359566]\n",
    "\n",
    "    for i in range(len(initState)):\n",
    "        pybullet.resetJointState(robotId, i, initState[i])\n",
    "\n",
    "    #set the gravity\n",
    "    pybullet.setGravity(0, 0, -10)\n",
    "\n",
    "    # create a thread to run the simulation\n",
    "    pybullet.setRealTimeSimulation(1)\n",
    "    \n",
    "    return robotId\n",
    "\n",
    "robotId = reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for controlling the robot arm\n",
    "isGrabberOpen = False\n",
    "def openGrabber():\n",
    "    global isGrabberOpen\n",
    "    for i in FINGER_AXES:\n",
    "        pybullet.setJointMotorControl2(robotId, i, pybullet.POSITION_CONTROL, targetPosition=0.04, force=100)\n",
    "    isGrabberOpen = True\n",
    "\n",
    "def closeGrabber():\n",
    "    global isGrabberOpen\n",
    "    for i in FINGER_AXES:\n",
    "        pybullet.setJointMotorControl2(robotId, i, pybullet.POSITION_CONTROL, targetPosition=0, force=100)\n",
    "    isGrabberOpen = False\n",
    "\n",
    "def setEndEffectorPos(pos, orientation):\n",
    "    #first get desired joint positions via inverse kinematics\n",
    "    jointPoses = pybullet.calculateInverseKinematics(robotId, 8, pos, orientation)\n",
    "    for i in range(len(jointPoses)):\n",
    "        if i in FINGER_AXES:\n",
    "            continue\n",
    "            \n",
    "        pybullet.setJointMotorControl2(robotId, i, pybullet.POSITION_CONTROL, targetPosition=jointPoses[i], force=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "def getIsOpen():\n",
    "    global isGrabberOpen\n",
    "    return isGrabberOpen\n",
    "\n",
    "def getPointsOnEndEffector(visualize=False):\n",
    "    linkNum = 8\n",
    "    #cet center position\n",
    "    endEffectorPos, endEffectorOrn = pybullet.getLinkState(robotId, linkNum)[:2]\n",
    "    #use orientation to compute the other two points\n",
    "    rotMat = pybullet.getMatrixFromQuaternion(endEffectorOrn)\n",
    "    rotMat = np.array(rotMat).reshape(3,3)\n",
    "    localPos = np.array([0, 0.2, 0])\n",
    "    endEffectorPos2 = endEffectorPos + rotMat.dot(localPos)\n",
    "    localPos = np.array([0.2, 0, 0])\n",
    "    endEffectorPos3 = endEffectorPos + rotMat.dot(localPos)\n",
    "\n",
    "    endState = np.array(endEffectorPos)\n",
    "    endState = np.append(endState, pybullet.getEulerFromQuaternion(endEffectorOrn))\n",
    "\n",
    "    if visualize:\n",
    "        #add small spheres to visualize the points\n",
    "        pybullet.addUserDebugLine(endEffectorPos, endEffectorPos2, [1,0,0], 1, 0.1)\n",
    "        pybullet.addUserDebugLine(endEffectorPos, endEffectorPos3, [0,1,0], 1, 0.1)\n",
    "    \n",
    "    return [endEffectorPos, endEffectorPos2, endEffectorPos3], endState\n",
    "\n",
    "def getTrainingData():\n",
    "    '''return everything in a timestep needed for training (rgb img, depth mask, end effector pos, finger open)\n",
    "    '''\n",
    "\n",
    "    viewMatrix = pybullet.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0,0.5], distance=1.1, yaw=0, pitch=-45, roll=0, upAxisIndex=2)\n",
    "    projectionMatrix = pybullet.computeProjectionMatrixFOV(fov=60, aspect=float(IMG_WIDTH)/IMG_HEIGHT, nearVal=0.1, farVal=100.0)\n",
    "    image = pybullet.getCameraImage(IMG_WIDTH, IMG_HEIGHT, viewMatrix=viewMatrix, projectionMatrix=projectionMatrix, shadow=1, lightDirection=[1,1,1], lightColor=[1,1,1], lightDistance=1, lightAmbientCoeff=0.5, lightDiffuseCoeff=0.5, lightSpecularCoeff=0.5, renderer=pybullet.ER_BULLET_HARDWARE_OPENGL, flags=pybullet.ER_NO_SEGMENTATION_MASK)\n",
    "\n",
    "    #show the image in frame\n",
    "    width = image[0]\n",
    "    height = image[1]\n",
    "    rgb = image[2]\n",
    "    depth = image[3]\n",
    "    segmentation = image[4]\n",
    "\n",
    "    #convert to numpy array\n",
    "    rgb = np.reshape(rgb, (height, width, 4))\n",
    "    rgb = rgb[:, :, :3] #remove alpha channel\n",
    "    \n",
    "    depth = np.reshape(depth, (height, width))\n",
    "\n",
    "    #get the 3 points on the end effector\n",
    "    endEffectorPoints, endState = getPointsOnEndEffector()\n",
    "\n",
    "    #get if the grabber is open\n",
    "    isOpen = getIsOpen()\n",
    "    \n",
    "    return rgb, depth, endEffectorPoints, endState, isOpen\n",
    "\n",
    "#visualize sample image\n",
    "rgb, depth, endEffectorPoints, endState, isOpen = getTrainingData()\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "plt.imshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCollectingImgs = False\n",
    "def continuousRecordImgs10Hz():\n",
    "    global runNumber\n",
    "    rgbImgs = []\n",
    "    depthImgs = []\n",
    "    points = []\n",
    "    opens = []\n",
    "    endStates = []\n",
    "    while isCollectingImgs:\n",
    "        startTime = time.time()\n",
    "        rgb, depth, endEffectorPoints, endState, isOpen = getTrainingData()\n",
    "        rgbImgs.append(rgb)\n",
    "        depthImgs.append(depth)\n",
    "        points.append(endEffectorPoints)\n",
    "        endStates.append(endState)\n",
    "        opens.append(isOpen)\n",
    "        endTime = time.time()\n",
    "        sleepTime = 0.1 - (endTime - startTime)\n",
    "        if sleepTime > 0:\n",
    "            time.sleep(sleepTime)\n",
    "        else:\n",
    "            print(\"Warning: Data collection is too slow, took\", endTime - startTime, \"seconds to collect one image\")\n",
    "    \n",
    "    #save the images\n",
    "    dataPath = f\"{DATA_PATH}/{runNumber}\"\n",
    "    os.mkdir(dataPath)\n",
    "    rgbPath = f\"{dataPath}/rgb\"\n",
    "    depthPath = f\"{dataPath}/depth\"\n",
    "    statePath = f\"{dataPath}/states\"\n",
    "    os.mkdir(rgbPath)\n",
    "    os.mkdir(depthPath)\n",
    "    os.mkdir(statePath)\n",
    "\n",
    "    for i in range(len(rgbImgs)):\n",
    "        #convert to bgr for opencv\n",
    "        bgr = rgbImgs[i][:, :, ::-1]\n",
    "        cv2.imwrite(f\"{rgbPath}/rgb{i}.png\", bgr)\n",
    "    \n",
    "    for i in range(len(depthImgs)):\n",
    "        #convert to int\n",
    "        img = (depthImgs[i] * 255).astype(np.uint8)\n",
    "        cv2.imwrite(f\"{depthPath}/depth{i}.png\", img)\n",
    "\n",
    "    #create a csv file with the points and opens for each time step\n",
    "    for i in range(len(points)):\n",
    "        with open(f\"{statePath}/states{i}.csv\", \"w\") as f:\n",
    "            f.write(f'{points[i][0][0]},{points[i][0][1]}')\n",
    "            f.write(f',{points[i][1][0]},{points[i][1][1]}')\n",
    "            f.write(f',{points[i][2][0]},{points[i][2][1]}')\n",
    "            if opens[i]:\n",
    "                f.write(\",1\")\n",
    "            else:\n",
    "                f.write(\",0\")\n",
    "            f.write('\\n')\n",
    "            f.write(f'{endStates[i][0]},{endStates[i][1]},{endStates[i][2]},{endStates[i][3]},{endStates[i][4]},{endStates[i][5]}')      \n",
    "\n",
    "def stopRun():\n",
    "    global runNumber\n",
    "    global isCollectingImgs\n",
    "\n",
    "    #stop data collection for this run\n",
    "    isCollectingImgs = False\n",
    "    time.sleep(0.2) #wait for the thread to stop\n",
    "\n",
    "def startNextRun():\n",
    "    global runNumber\n",
    "    global isCollectingImgs\n",
    "    #start the next run\n",
    "    runNumber += 1\n",
    "    isCollectingImgs = True\n",
    "    thread = threading.Thread(target=continuousRecordImgs10Hz)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kill ongoing threads\n",
    "isRunning = False\n",
    "isCollectingImgs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isRunning = True\n",
    "# pybullet.setVRCameraState(trackObject=robotId)\n",
    "def updateControllerContinuously():\n",
    "    global robotId\n",
    "    \n",
    "    # Button A -> controllerState[6][7]\n",
    "    # Button B -> controllerState[6][1]\n",
    "    # Front trigger -> controllerState[6][33]\n",
    "    # Side trigger -> controllerState[6][34] or controllerState[6][2]\n",
    "\n",
    "    # For starting recording use A and Front trigger together\n",
    "    # For stopping recording use B and Front trigger together\n",
    "\n",
    "    is_recording = 0\n",
    "\n",
    "    while isRunning:\n",
    "\n",
    "        #get the position and orientation of the controller\n",
    "        controllerState = pybullet.getVREvents()\n",
    "        \n",
    "        if len(controllerState)!=2:\n",
    "            continue\n",
    "        \n",
    "        #For right controller, use 2nd set\n",
    "        controllerState = controllerState[1]\n",
    "        \n",
    "        #if trigger is pressed, close the grabber\n",
    "        if controllerState[6][33] == 1:\n",
    "            closeGrabber()\n",
    "        else:\n",
    "            openGrabber()\n",
    "\n",
    "        pos = controllerState[1]\n",
    "        orientation = controllerState[2]\n",
    "\n",
    "        #rotate orientation by 90 degrees in the x axis\n",
    "        orientation = pybullet.getEulerFromQuaternion(orientation)\n",
    "        orientation = list(orientation)\n",
    "        orientation[0] += math.pi\n",
    "        orientation = pybullet.getQuaternionFromEuler(orientation)\n",
    "        \n",
    "        setEndEffectorPos(pos, orientation)\n",
    "\n",
    "        #convert to tuple\n",
    "        orientation = list(orientation)\n",
    "\n",
    "        if controllerState[6][7]!=0 and controllerState[6][33]!=0:\n",
    "            time.sleep(10)\n",
    "            startNextRun()\n",
    "            is_recording = 1\n",
    "            print('Recording initiated...')\n",
    "        \n",
    "        if controllerState[6][1]!=0 and controllerState[6][33]!=0:\n",
    "            stopRun()\n",
    "            print('Recording stopped...')\n",
    "            time.sleep(5) #stop mid-reset pictures from being recorded\n",
    "            # robotId = reset()\n",
    "            # time.sleep(5)\n",
    "\n",
    "#start the controller thread\n",
    "thread = threading.Thread(target=updateControllerContinuously)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updateControllerContinuously()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kill the thread\n",
    "isRunning = False\n",
    "isCollectingImgs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the simulation\n",
    "pybullet.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control the robot arm with a deep learning model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VRNet import VRNet\n",
    "from VRNet import DataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VRNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model_11_500epoch_0.0005lr.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "#setup data preprocesser\n",
    "\n",
    "# rgb mean:  tensor([0.5231, 0.5100, 0.4927])\n",
    "# rgb std:  tensor([0.1867, 0.2083, 0.2512])\n",
    "# depth mean:  tensor([0.8959])\n",
    "# depth std:  tensor([0.0430])\n",
    "# states mean:  tensor([ 0.0081, -0.0053,  0.0088,  0.0304,  0.0053,  0.0292,  0.5139],\n",
    "#        device='cuda:0')\n",
    "# states std:  tensor([0.0215, 0.0279, 0.0781, 0.3493, 0.0188, 0.1732, 0.4998],\n",
    "#        device='cuda:0')\n",
    "\n",
    "\n",
    "preprocessor = DataPreprocessor(rgb_mean=torch.tensor([0.5231, 0.5100, 0.4927]), rgb_std=torch.tensor([0.1867, 0.2083, 0.2512]), depth_mean=torch.tensor([0.8959]), depth_std=torch.tensor([0.0430]), \\\n",
    "                        state_mean=torch.tensor([ 0.0081, -0.0053,  0.0088,  0.0304,  0.0053,  0.0292,  0.5139]), state_std=torch.tensor([0.0215, 0.0279, 0.0781, 0.3493, 0.0188, 0.1732, 0.4998]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentEndEffectorPose():\n",
    "    global robotId\n",
    "    pos, orientation = pybullet.getLinkState(robotId, 8, computeForwardKinematics=False)[0:2]\n",
    "    z_off = 0.04\n",
    "    pos = [pos[0], pos[1], pos[2] + z_off]\n",
    "    return pos, orientation\n",
    "\n",
    "#in a separate thread, continuously update the robot arm with the model\n",
    "modelControllingRobot = True\n",
    "vels = []\n",
    "def updateRobotNN():\n",
    "    startTime = time.time()\n",
    "    rgb, depth, _, _, _ = getTrainingData()\n",
    "\n",
    "    rgb = torch.from_numpy(rgb).permute(2, 0, 1)\n",
    "    depth = torch.from_numpy(depth)\n",
    "    rgb = rgb.unsqueeze(0).to(device).float() / 255\n",
    "    depth = depth.unsqueeze(0).unsqueeze(0).to(device).float()\n",
    "\n",
    "    #show images with matplotlib\n",
    "    # plt.imshow(rgb[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "    # plt.imshow(depth[0].detach().cpu().numpy())\n",
    "    # plt.show()\n",
    "\n",
    "    rgb_norm = preprocessor.normalizeRgb(rgb.cpu()).to(device)\n",
    "    depth_norm = preprocessor.normalizeDepth(depth.cpu()).to(device)\n",
    "    y = model(rgb_norm, depth_norm)\n",
    "\n",
    "    #convert the output to a list of velocities\n",
    "    outputs = y.detach().cpu()\n",
    "    outputs = preprocessor.denormalizeState(outputs)\n",
    "    outputs = outputs[0].numpy()\n",
    "    \n",
    "    Vx, Vy, Vz, Wx, Wy, Wz, grabberOpen = outputs\n",
    "\n",
    "    #compute the next position of the end effector in 100 ms\n",
    "    pos, orn = getCurrentEndEffectorPose()\n",
    "    x, y, z = pos\n",
    "    \n",
    "    x += Vx * 0.5 #use 0.5 instead of 0.1 to make the robot move faster for demos\n",
    "    y += Vy * 0.5\n",
    "    z += Vz * 0.5\n",
    "    # roll += Wx * 0.01 #for simplicity, no angular control\n",
    "    # pitch += Wy * 0.01\n",
    "    # yaw += Wz * 0.01\n",
    "    # vels.append([Vx, Vy, Vz, rgb[0].permute(1, 2, 0).detach().cpu().numpy()])\n",
    "\n",
    "    pos = np.array([x, y, z])\n",
    "    orn = pybullet.getQuaternionFromEuler([0, math.pi, -math.pi/1.7])\n",
    "\n",
    "    #move the robot arm to the next position\n",
    "    setEndEffectorPos(pos, orn)\n",
    "\n",
    "    #open or close the grabber\n",
    "    if grabberOpen > 0.5:\n",
    "        openGrabber()\n",
    "    else:\n",
    "        closeGrabber()\n",
    "    \n",
    "    dt = time.time() - startTime\n",
    "    if dt < 0.1:\n",
    "        time.sleep(0.1 - dt)\n",
    "    else:\n",
    "        print(\"Warning: updateRobotNN took too long: \", dt)\n",
    "\n",
    "def updateRobotContinuous():\n",
    "    while modelControllingRobot:\n",
    "        updateRobotNN()\n",
    "\n",
    "#start the thread\n",
    "thread = threading.Thread(target=updateRobotContinuous)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelControllingRobot = False\n",
    "reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85eab4bc6c6c5de8d8c9b73424489217eb3ac6fe3bc76caf6b2eb1d4f520884f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
